{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, pickle\n",
    "from joblib import Parallel, delayed\n",
    "from gensim.models import KeyedVectors\n",
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "from scipy.spatial.distance import cosine, euclidean\n",
    "from scipy.stats import pearsonr\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# import tensorflow as tf\n",
    "# import tensorflow.contrib.eager as tfe\n",
    "# tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fns_and_meta(data_pth, folders):\n",
    "    \"\"\" load the word vector model filenames\n",
    "    \"\"\"\n",
    "    models_meta = {}\n",
    "    for folder in folders:\n",
    "        for file in os.listdir(os.path.join(data_pth, \n",
    "                                            folder)):\n",
    "            model_meta = {}\n",
    "            model_meta['root'] = data_pth\n",
    "            model_meta['class'] = folder\n",
    "            model_meta['fn'] = file\n",
    "            #model_meta['name'] = file[:-4]\n",
    "            \n",
    "            if folder == 'glove':\n",
    "                dim = file[file.find('B')+2:file.find('d')]\n",
    "                model_meta['d'] = int(dim)\n",
    "            elif folder == 'w2v':\n",
    "                model_meta['d'] = 300\n",
    "            \n",
    "            models_meta[file[:-4]] = model_meta\n",
    "    \n",
    "    return models_meta\n",
    "\n",
    "def load_relsim_data(path='', fn='relsim_mean_ratings.csv'):\n",
    "\n",
    "    df = pd.read_csv(path+fn)\n",
    "    df['rel1_type'] = df['relation1'].apply(lambda x: int(x[:-1]))\n",
    "    df['rel2_type'] = df['relation2'].apply(lambda x: int(x[:-1]))\n",
    "    \n",
    "    return df\n",
    "\n",
    "def words_in_vocab(words, model):\n",
    "    \n",
    "    status = True\n",
    "    for w in words:\n",
    "        try:\n",
    "            if w not in model.vocab:\n",
    "                status = False\n",
    "        except:\n",
    "            if w not in model.keys():\n",
    "                status = False            \n",
    "    return status\n",
    "\n",
    "\n",
    "def compute_similarity(u, v, metric='e'):\n",
    "    \n",
    "    if metric in ['inner product', 'ip']:\n",
    "        return np.dot(u, v)\n",
    "    \n",
    "    elif metric in ['cosine', 'c']:\n",
    "        return 1 - cosine(u, v)\n",
    "    \n",
    "    elif metric in ['euclidean', 'e']:\n",
    "        return -euclidean(u, v)\n",
    "    \n",
    "    elif metric in ['dawn_euclidean', 'd']:\n",
    "        return 1 - euclidean(u, v)\n",
    "\n",
    "    \n",
    "def get_analogy_words(trial):\n",
    "    \n",
    "    return [trial.pair1_word1,\n",
    "            trial.pair1_word2,\n",
    "            trial.pair2_word1,\n",
    "            trial.pair2_word2]\n",
    "\n",
    "\n",
    "def get_word_vector(word, model, normalize=True):\n",
    "    \n",
    "    word_vector = model[word]\n",
    "    \n",
    "    if normalize:\n",
    "        return word_vector / norm(word_vector)\n",
    "    else:       \n",
    "        return word_vector\n",
    "\n",
    "\n",
    "def get_relsim_vocab(df):\n",
    "    \n",
    "    words = []\n",
    "    words += list(df.pair1_word1.unique())\n",
    "    words += list(df.pair1_word2.unique())\n",
    "    words += list(df.pair2_word1.unique())\n",
    "    words += list(df.pair2_word2.unique())\n",
    "    \n",
    "    return list(set(words))\n",
    "\n",
    "\n",
    "def create_condensed_model_relsim(df, model):\n",
    "    \"\"\" Create a condensed model made just\n",
    "        for the relational similarity data.\n",
    "    \"\"\"\n",
    "    \n",
    "    vocab = get_relsim_vocab(df)\n",
    "    \n",
    "    return create_condensed_model(vocab, model)\n",
    "\n",
    "\n",
    "def create_condensed_model(vocab, model):\n",
    "    \"\"\" Create a condensed model as a {word: vector} \n",
    "        dictionary object for a smaller vocabulary\n",
    "        from an input w2v gensim model.\n",
    "    \"\"\"\n",
    "    condensed_model = {}\n",
    "    \n",
    "    for word in vocab:\n",
    "        if word in model.vocab:\n",
    "            condensed_model[word] = model[word]\n",
    "        \n",
    "    return condensed_model\n",
    "\n",
    "\n",
    "def load_model(model_fn='GoogleNews-vectors-negative300.bin',\n",
    "               data_pth = '../../../../datasets/word-vector-datasets/',\n",
    "               binary=True, load_condensed_stem=None, \n",
    "               condensed_vocab=None, save_condensed=False, \n",
    "               condensed_path=None):\n",
    "    \n",
    "    \"\"\" load word vector model w/ gensim\n",
    "    \"\"\"\n",
    "    \n",
    "    if 'glove' in model_fn:\n",
    "        binary = False\n",
    "        data_pth += 'glove/'\n",
    "    elif 'GoogleNews' in model_fn:\n",
    "        data_pth += 'w2v/'\n",
    "        \n",
    "    if None not in [load_condensed_stem, condensed_vocab, condensed_path]:\n",
    "        c_model_fn = model_fn[:-3] + load_condensed_stem\n",
    "        c_model_path = condensed_path + c_model_fn\n",
    "        \n",
    "        if os.path.isfile(c_model_path):\n",
    "            return pickle.load(open(c_model_path, \"rb\"))\n",
    "        else:\n",
    "            model = KeyedVectors.load_word2vec_format(data_pth + model_fn, \n",
    "                                                      binary=binary)\n",
    "            c_model = create_condensed_model(condensed_vocab, model)\n",
    "            if save_condensed: pickle.dump(c_model, open(c_model_path, \"wb\"))\n",
    "            return c_model\n",
    "    else:\n",
    "        return KeyedVectors.load_word2vec_format(data_pth + model_fn, \n",
    "                                                 binary=binary)\n",
    "\n",
    "\n",
    "def get_diff_vecs(words, model, dims=None):\n",
    "    \n",
    "    w1_vec = get_word_vector(words[0], model)\n",
    "    w2_vec = get_word_vector(words[1], model)\n",
    "    w3_vec = get_word_vector(words[2], model)\n",
    "    w4_vec = get_word_vector(words[3], model)\n",
    "    \n",
    "    diff_pair1 = w1_vec - w2_vec\n",
    "    diff_pair2 = w3_vec - w4_vec\n",
    "    \n",
    "    if dims is None:\n",
    "        return diff_pair1, diff_pair2\n",
    "    else:\n",
    "        return diff_pair1[dims], diff_pair2[dims]\n",
    "    \n",
    "\n",
    "def naive_train_test_split(df, val_percent=0.2, \n",
    "                           shuffle=True, seed=1):\n",
    "    \"\"\" Doesn't avoid shared single words\n",
    "        across train and test sets!!\n",
    "    \"\"\"\n",
    "    train_percent = 1 - val_percent\n",
    "    \n",
    "    n = df.shape[0]\n",
    "    idxs = np.arange(n)\n",
    "    np.random.seed(seed)\n",
    "    if shuffle: np.random.shuffle(idxs)\n",
    "    \n",
    "    train_idxs = idxs[:int(n*train_percent)]\n",
    "    val_idxs = idxs[int(n*train_percent):]\n",
    "    \n",
    "    return train_idxs, val_idxs\n",
    "\n",
    "\n",
    "def score_preds(df):\n",
    "    return pearsonr(df[df.in_vocab==True].mean_rating, \n",
    "                    df[df.in_vocab==True].preds)\n",
    "\n",
    "\n",
    "def get_rel_sim_preds(df, model, dims=None,\n",
    "                      metric='e'):\n",
    "    \n",
    "    preds, in_vocab = [], []\n",
    "    for r, row in df.iterrows():\n",
    "        \n",
    "        words = get_analogy_words(row)\n",
    "        \n",
    "        if words_in_vocab(words, model):\n",
    "        \n",
    "            diff_pair1, diff_pair2 = \\\n",
    "                get_diff_vecs(words, model, dims=dims)\n",
    "            \n",
    "            sim = compute_similarity(diff_pair1, diff_pair2,\n",
    "                                     metric=metric)\n",
    "            preds.append(sim)\n",
    "            in_vocab.append(True)\n",
    "        else:\n",
    "            preds.append(999)\n",
    "            in_vocab.append(False)\n",
    "        \n",
    "    df['preds'] = preds\n",
    "    df['in_vocab'] = in_vocab\n",
    "    return df\n",
    "\n",
    "def search_for_best_axes(df, model, epsilon=0, verbose=0):\n",
    "    \"\"\" Find the subset of dimensions (axis-aligned subspace)\n",
    "        giving the best fit to human data.\n",
    "    \"\"\"\n",
    "    \n",
    "    n_feats = model['dog'].size\n",
    "    feat_idx_keep = np.arange(n_feats)\n",
    "    \n",
    "    df_pred = get_rel_sim_preds(df, model)\n",
    "    base_score = score_preds(df_pred)[0]\n",
    "    best_score = base_score\n",
    "    if verbose > 0:\n",
    "        print('Base Score : %.4f, Features: %i' % (best_score, n_feats))\n",
    "    \n",
    "    for feat_idx in np.arange(n_feats):\n",
    "        \n",
    "        curr_feat_set_proposal = feat_idx_keep[feat_idx_keep!=feat_idx]\n",
    "\n",
    "        df_pred = get_rel_sim_preds(df, model, dims=curr_feat_set_proposal)\n",
    "        curr_score = score_preds(df_pred)[0]\n",
    "        \n",
    "        if (curr_score > best_score) and (curr_score-best_score > epsilon):\n",
    "            best_score = curr_score\n",
    "            feat_idx_keep = curr_feat_set_proposal\n",
    "            if verbose > 1:\n",
    "                print('-- New Best: %.4f, Features: %i' % (best_score, feat_idx_keep.size))\n",
    "                \n",
    "    if verbose > 0:                \n",
    "        print('Final Score: %.4f, Features: %i' % (best_score, feat_idx_keep.size))\n",
    "            \n",
    "    return feat_idx_keep, base_score, best_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load human relational similarity data\n",
    "df_rel_sim = load_relsim_data()\n",
    "\n",
    "# get the vocab for the dataset\n",
    "vocab = get_relsim_vocab(df_rel_sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = load_model(model_fn='glove.6B.50d.w2vformat.txt',\n",
    "#                    data_pth = '../../../../datasets/word-vector-datasets/',\n",
    "#                    binary=True, load_condensed_stem='relsim.condensed.p', \n",
    "#                    condensed_vocab=vocab, save_condensed=True, \n",
    "#                    condensed_path='condensed_models/')\n",
    "\n",
    "data_pth = '../../../../datasets/word-vector-datasets/'\n",
    "folders = ['glove','w2v']\n",
    "\n",
    "models = get_fns_and_meta(data_pth, folders)\n",
    "\n",
    "# store all condensed models in one dict\n",
    "for model_key in models.keys():\n",
    "    \n",
    "    models[model_key]['model'] = load_model(model_fn=models[model_key]['fn'],\n",
    "           data_pth=models[model_key]['root'],\n",
    "           binary=True, load_condensed_stem='relsim.condensed.p', \n",
    "           condensed_vocab=vocab, save_condensed=True, \n",
    "           condensed_path='condensed_models/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# load word vector model w/ gensim\n",
    "\n",
    "# model_fn = 'glove.6B.50d.w2vformat.txt'\n",
    "# model_fn = 'glove.840B.300d.w2vformat.txt'\n",
    "model_fn = 'GoogleNews-vectors-negative300.bin'\n",
    "\n",
    "binary = True # will mark false below if needed\n",
    "data_pth = '../../../../datasets/word-vector-datasets/'\n",
    "\n",
    "if 'glove' in model_fn:\n",
    "    fn_stem = -13\n",
    "    binary = False\n",
    "    data_pth += 'glove/'\n",
    "else:\n",
    "    fn_stem = -3\n",
    "    data_pth += 'w2v/'\n",
    "\n",
    "c_model_fn = model_fn[:fn_stem]+'relsim.condensed.p'\n",
    "c_model_path = 'condensed_models/' + c_model_fn\n",
    "if not os.path.isfile(c_model_path):\n",
    "    model = KeyedVectors.load_word2vec_format(data_pth + model_fn, \n",
    "                                              binary=binary)\n",
    "\n",
    "# load the human data\n",
    "df_rel_sim = load_relsim_data()\n",
    "\n",
    "if not os.path.isfile(c_model_path):\n",
    "    c_model = create_condensed_model(df_rel_sim, model)\n",
    "    pickle.dump(c_model, open(c_model_path, \"wb\"))\n",
    "else:\n",
    "    c_model = pickle.load(open(c_model_path, \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic analysis\n",
    "df_rel_sim = get_rel_sim_preds(df_rel_sim, c_model)\n",
    "\n",
    "print(score_preds(df_rel_sim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# search for a subset of dimensions with best\n",
    "# overall score across all types/subtypes\n",
    "\n",
    "search_for_best_axes(df_rel_sim, model, epsilon=0.0001, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# search for a subset of dimensions with best\n",
    "# overall score across all types/subtypes\n",
    "\n",
    "n_splits = 10\n",
    "epsilon = 0.0001\n",
    "\n",
    "all_base_scores = []\n",
    "all_best_scores = []\n",
    "\n",
    "train_base_scores = []\n",
    "train_best_scores = []\n",
    "\n",
    "val_base_scores = []\n",
    "val_best_scores = []\n",
    "\n",
    "for rel_type in range(1, 11):\n",
    "    \n",
    "    # within-TYPE trials only (what Dawn did for paper!)\n",
    "    exp_params = (df_rel_sim.rel1_type==rel_type) & (df_rel_sim.rel2_type==rel_type)\n",
    "    \n",
    "    df_exp = df_rel_sim[exp_params].copy()\n",
    "    \n",
    "    print('Type', rel_type, ' - All Data Score', df_exp.shape[0])\n",
    "\n",
    "    feats_all_data, all_base_score, all_best_score = \\\n",
    "        search_for_best_axes(df_exp, model, verbose=1, epsilon=epsilon)\n",
    "    all_base_scores.append(all_base_score)\n",
    "    all_best_scores.append(all_best_score)\n",
    "    print('')\n",
    "    \n",
    "    avg_train_base_scores = []\n",
    "    avg_train_best_scores = []\n",
    "    avg_val_base_scores = []\n",
    "    avg_val_best_scores = []\n",
    "    \n",
    "    for split in range(n_splits):\n",
    "        train_idxs, val_idxs = naive_train_test_split(df_exp, \n",
    "                                                      val_percent=0.2, \n",
    "                                                      shuffle=True)\n",
    "\n",
    "    #     print('Type', rel_type, ' - Training Score', \n",
    "    #           df_exp.iloc[train_idxs].shape[0])\n",
    "\n",
    "        feats_train, train_base_score, train_best_score = \\\n",
    "            search_for_best_axes(df_exp.iloc[train_idxs].copy(), \n",
    "                                 model, verbose=0, epsilon=epsilon)\n",
    "        \n",
    "        df_val_base = get_rel_sim_preds(df_exp.iloc[val_idxs].copy(), model)\n",
    "        df_val = get_rel_sim_preds(df_exp.iloc[val_idxs].copy(), model, dims=feats_train)\n",
    "        print(score_preds(df_val)[0])\n",
    "        \n",
    "        avg_train_base_scores.append(train_base_score)\n",
    "        avg_train_best_scores.append(train_best_score)\n",
    "        avg_val_base_scores.append(score_preds(df_val_base)[0])\n",
    "        avg_val_best_scores.append(score_preds(df_val)[0])\n",
    "        \n",
    "    print('mean val', np.mean(avg_val_best_scores))\n",
    "    \n",
    "    train_base_scores.append(np.mean(avg_train_base_scores))\n",
    "    train_best_scores.append(np.mean(avg_train_best_scores))\n",
    "    val_base_scores.append(np.mean(avg_val_base_scores))\n",
    "    val_best_scores.append(np.mean(avg_val_best_scores))\n",
    "#     print('')\n",
    "    \n",
    "#     df_val = get_rel_sim_preds(df_exp.iloc[val_idxs].copy(), model, dims=feats_train)\n",
    "\n",
    "#     print('Type', rel_type, ' - Validation Score', \n",
    "#           df_exp.iloc[val_idxs].shape[0])\n",
    "#     print('%.4f' % score_preds(df_val)[0])\n",
    "    print('')\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### JOBLIB TEST!!! ###\n",
    "\n",
    "# search for a subset of dimensions with best\n",
    "# overall score across all types/subtypes\n",
    "\n",
    "condensed_model = create_condensed_model(df_rel_sim, model)\n",
    "\n",
    "n_splits = 50\n",
    "epsilon = 0.0001\n",
    "\n",
    "all_base_scores = []\n",
    "all_best_scores = []\n",
    "\n",
    "train_base_scores = []\n",
    "train_best_scores = []\n",
    "\n",
    "val_base_scores = []\n",
    "val_best_scores = []\n",
    "\n",
    "for rel_type in range(1, 11):\n",
    "    \n",
    "    # within-TYPE trials only (what Dawn did for paper!)\n",
    "    exp_params = (df_rel_sim.rel1_type==rel_type) & (df_rel_sim.rel2_type==rel_type)\n",
    "    \n",
    "    df_exp = df_rel_sim[exp_params].copy()\n",
    "    \n",
    "    print('Type', rel_type, ' - All Data Score', df_exp.shape[0])\n",
    "\n",
    "    feats_all_data, all_base_score, all_best_score = \\\n",
    "        search_for_best_axes(df_exp, model, verbose=0, epsilon=epsilon)\n",
    "    all_base_scores.append(all_base_score)\n",
    "    all_best_scores.append(all_best_score)\n",
    "#     print('')\n",
    "    \n",
    "    avg_train_base_scores = []\n",
    "    avg_train_best_scores = []\n",
    "    avg_val_base_scores = []\n",
    "    avg_val_best_scores = []\n",
    "    \n",
    "    def run_split(seed, df_exp):\n",
    "        train_idxs, val_idxs = naive_train_test_split(df_exp, \n",
    "                                                      val_percent=0.2,\n",
    "                                                      shuffle=True,\n",
    "                                                      seed=seed)\n",
    "\n",
    "        feats_train, train_base_score, train_best_score = \\\n",
    "            search_for_best_axes(df_exp.iloc[train_idxs].copy(), \n",
    "                                 condensed_model, verbose=0, epsilon=epsilon)\n",
    "        \n",
    "        df_val_base = get_rel_sim_preds(df_exp.iloc[val_idxs].copy(), condensed_model)\n",
    "        val_base_score = score_preds(df_val_base)[0]\n",
    "        \n",
    "        df_val = get_rel_sim_preds(df_exp.iloc[val_idxs].copy(), condensed_model, dims=feats_train)\n",
    "        val_best_score = score_preds(df_val)[0]\n",
    "        \n",
    "        return train_base_score, train_best_score, val_base_score, val_best_score\n",
    "    \n",
    "    results = Parallel(n_jobs=n_splits)(delayed(run_split)(i, df_exp) for i in range(n_splits))\n",
    "    for result in results: print(result)\n",
    "    \n",
    "    for result in results:        \n",
    "        avg_train_base_scores.append(result[0])\n",
    "        avg_train_best_scores.append(result[1])\n",
    "        avg_val_base_scores.append(result[2])\n",
    "        avg_val_best_scores.append(result[3])\n",
    "        \n",
    "    print('mean val', np.mean(avg_val_best_scores))\n",
    "    \n",
    "    train_base_scores.append(np.mean(avg_train_base_scores))\n",
    "    train_best_scores.append(np.mean(avg_train_best_scores))\n",
    "    val_base_scores.append(np.mean(avg_val_base_scores))\n",
    "    val_best_scores.append(np.mean(avg_val_best_scores))\n",
    "\n",
    "    print('')\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Type 1 epsilon test\n",
    "121 0       0.5298 0.4289\n",
    "121 0.00001 0.5298 0.4127\n",
    "127 0.0001  0.5230 0.4552\n",
    "204 0.001   0.4023 0.3364\n",
    "300 0.01    0.1511 NA\n",
    "300 0.1     0.1511 NA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for rel_type in range(1, 11):\n",
    "    \n",
    "    # within-TYPE trials only (what Dawn did for paper!)\n",
    "    exp_params = (df_rel_sim.rel1_type==rel_type) & (df_rel_sim.rel2_type==rel_type)\n",
    "    \n",
    "    df_exp = df_rel_sim[exp_params].copy()\n",
    "    \n",
    "    print('Type', rel_type, ' - All Data Score', df_exp.shape[0])\n",
    "    df_exp = get_rel_sim_preds(df_exp, model, metric='e')\n",
    "\n",
    "    print(score_preds(df_exp)[0])\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(dpi=150)\n",
    "\n",
    "# set width of bar\n",
    "barWidth = 0.25\n",
    " \n",
    "# set height of bar\n",
    "bars1 = all_base_scores\n",
    "bars2 = all_best_scores\n",
    "bars3 = val_best_scores\n",
    " \n",
    "# Set position of bar on X axis\n",
    "r1 = np.arange(len(bars1))\n",
    "r2 = [x + barWidth for x in r1]\n",
    "r3 = [x + barWidth for x in r2]\n",
    " \n",
    "# Make the plot\n",
    "plt.bar(r1, bars1, color='black', width=barWidth, edgecolor='white', \n",
    "        label='Original GloVe')\n",
    "plt.axhline(y=np.mean(bars1), color='black', linestyle='--')\n",
    "plt.bar(r2, bars2, color='#2d7f5e', width=barWidth, edgecolor='white', \n",
    "        label='Best Subspace (All Data)')\n",
    "plt.axhline(y=np.mean(bars2), color='#2d7f5e', linestyle='--')\n",
    "plt.bar(r3, bars3, color='purple', width=barWidth, edgecolor='white', \n",
    "        label='Best Subspace (Mean 10x Validation)')\n",
    "plt.axhline(y=np.mean(bars3), color='purple', linestyle='--')\n",
    " \n",
    "# Add xticks on the middle of the group bars\n",
    "# plt.xlabel('group', fontweight='bold')\n",
    "plt.ylabel('Pearson $r$', fontweight='bold')\n",
    "plt.xlabel('Relation Type', fontweight='bold')\n",
    "plt.xticks([r + barWidth for r in range(len(bars1))], range(1, 11))\n",
    "\n",
    "plt.ylim([0,1])\n",
    "\n",
    "# Create legend & Show graphic\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_params = (df_rel_sim.rel1_type==2) & (df_rel_sim.rel2_type==2)\n",
    "\n",
    "df_rel_sim[exp_params]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(15,15))\n",
    "# fig, ax = plt.subplots(2, 5)\n",
    "# ax = ax.flatten()\n",
    "\n",
    "for rel_type in range(1, 11):\n",
    "    plt.figure()\n",
    "    \n",
    "    # within-TYPE trials only (what Dawn did for paper!)\n",
    "    exp_params = (df_rel_sim.rel1_type==rel_type) & (df_rel_sim.rel2_type==rel_type)\n",
    "    \n",
    "    df_exp = df_rel_sim[exp_params].copy()\n",
    "    \n",
    "    result = search_for_best_axes(df_exp, model, \n",
    "                                  epsilon=0.0001, verbose=0)\n",
    "    good_feats = result[0]\n",
    "    \n",
    "    for r, row in df_exp.iterrows():\n",
    "\n",
    "        words = get_analogy_words(row)\n",
    "\n",
    "        if words_in_vocab(words, model):\n",
    "\n",
    "            diff_pair1, diff_pair2 = \\\n",
    "                get_diff_vecs(words, model)\n",
    "\n",
    "            sim = compute_similarity(diff_pair1[good_feats], \n",
    "                                     diff_pair2[good_feats],\n",
    "                                     metric='e')\n",
    "        plt.scatter(row.mean_rating, -sim, \n",
    "                    s=10, color='blue', alpha=0.5)\n",
    "#         ax[rel_type-1].scatter(row.mean_rating, -sim, \n",
    "#                     s=10, color='blue', alpha=0.5)\n",
    "    print(rel_type)\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
